{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:14:07.998850400Z",
     "start_time": "2024-05-25T19:14:07.943949500Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import plotting\n",
    "from DQN_Agent import DQN_Agent\n",
    "from IDS import IDS_Agent\n",
    "from UCB import UCB_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:14:08.002839300Z",
     "start_time": "2024-05-25T19:14:07.958909500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FrozenLakeEnv(gym.Env):\n",
    "    def __init__(self, grid):\n",
    "        super(FrozenLakeEnv, self).__init__()\n",
    "        self.grid = grid\n",
    "        self.grid_height = len(grid)\n",
    "        self.grid_width = len(grid[0])\n",
    "        [self.start_y], [self.start_x] = np.where(np.array(grid) == 'S')\n",
    "        self.slip_probability = 1/3 # Probability to slip to one side (so the chance of slipping in any direction is 2 times this value)\n",
    "        assert self.slip_probability <= 1/2\n",
    "        self.action_space = spaces.Discrete(4)  # Left, Down, Right, Up\n",
    "        self.observation_space = spaces.Discrete(self.grid_height * self.grid_width)\n",
    "\n",
    "        self.state_action_count = {}\n",
    "        for x in range(self.grid_width):\n",
    "            for y in range(self.grid_height):\n",
    "                self.state_action_count[(x, y)] = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "\n",
    "    def reset(self):\n",
    "        # Top left corner is 0, 0\n",
    "        self.state = (self.start_x, self.start_y)\n",
    "        return self.to_observation(self.state)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state_action_count[self.state][action] += 1\n",
    "        x, y = self.state\n",
    "\n",
    "        # Define possible actions for each chosen direction\n",
    "        # Make sure the first action in the array is the action itself (no slip)\n",
    "        possible_actions = {\n",
    "                0: [0, 3, 1],  # Left\n",
    "                1: [1, 0, 2],  # Down\n",
    "                2: [2, 1, 3],  # Right\n",
    "                3: [3, 2, 0]   # Up\n",
    "            }            \n",
    "\n",
    "        # Choose a random action from the possible actions according to self.slip_probability\n",
    "        p = self.slip_probability\n",
    "        action = np.random.choice(possible_actions[action], p=[1-2*p, p, p])\n",
    "        # print(\"Actual action\", [\"left\", \"down\", \"right\", \"up\"][action])\n",
    "\n",
    "        # Move in the chosen direction if its within bounds\n",
    "        if action == 0 and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 1 and y < self.grid_height - 1:\n",
    "            y += 1\n",
    "        elif action == 2 and x < self.grid_width - 1:\n",
    "            x += 1\n",
    "        elif action == 3 and y > 0:\n",
    "            y -= 1\n",
    "\n",
    "        self.state = (x, y)\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Check state of the cell\n",
    "        if self.grid[y][x] == 'X':\n",
    "            reward = -5\n",
    "            done = True\n",
    "        elif self.grid[y][x] == 'G':\n",
    "            reward = 10\n",
    "            done = True\n",
    "\n",
    "        return self.to_observation(self.state), reward, done, {}\n",
    "    \n",
    "    def get_aleatoric_uncertainty(self):\n",
    "        # Define possible actions for each chosen direction\n",
    "        # Make sure the first action in the array is the action itself (no slip)\n",
    "        possible_actions = {\n",
    "                0: [0, 3, 1],  # Left\n",
    "                1: [1, 0, 2],  # Down\n",
    "                2: [2, 1, 3],  # Right\n",
    "                3: [3, 2, 0]   # Up\n",
    "            }  \n",
    "        variance_map = []\n",
    "\n",
    "        # Choose a random action from the possible actions according to self.slip_probability\n",
    "        p = self.slip_probability\n",
    "\n",
    "        for state in range(self.grid_width*self.grid_height):\n",
    "            # Move in the chosen direction if its within bounds\n",
    "            variances = {0:[],1:[],2:[],3:[]}\n",
    "            # print(state)\n",
    "            for wanted_action in possible_actions:\n",
    "                for action in possible_actions[wanted_action]:\n",
    "                    x, y = state % self.grid_width, state // self.grid_height\n",
    "                    # print(x,y)\n",
    "                    if action == 0 and x > 0:\n",
    "                        x -= 1\n",
    "                    elif action == 1 and y < self.grid_height - 1:\n",
    "                        y += 1\n",
    "                    elif action == 2 and x < self.grid_width - 1:\n",
    "                        x += 1\n",
    "                    elif action == 3 and y > 0:\n",
    "                        y -= 1\n",
    "                    # Check state of the cell\n",
    "                    if self.grid[y][x] == 'X':\n",
    "                        reward = -1\n",
    "                    elif self.grid[y][x] == 'G':\n",
    "                        reward = 1\n",
    "                    else:\n",
    "                        reward = 0\n",
    "                    variances[wanted_action].append(reward)\n",
    "            # print(variances)\n",
    "            \n",
    "            for k in variances:\n",
    "                variances[k] = torch.tensor(np.var(variances[k]))\n",
    "            variances = torch.stack(tuple(variances[k] for k in variances))\n",
    "            variance_map.append(variances)\n",
    "\n",
    "        return variance_map\n",
    "\n",
    "\n",
    "    def to_observation(self, state):\n",
    "        x, y = state\n",
    "        return y * self.grid_width + x\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((self.grid_height, self.grid_width), ' ')\n",
    "        for y in range(self.grid_height):\n",
    "            for x in range(self.grid_width):\n",
    "                grid[y, x] = self.grid[y][x]\n",
    "        x, y = self.state\n",
    "        grid[y, x] = 'A'\n",
    "        print('\\n'.join(' '.join(row) for row in grid))\n",
    "        print()\n",
    "\n",
    "def make_frozenlake(grid):\n",
    "    return FrozenLakeEnv(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:14:08.305469200Z",
     "start_time": "2024-05-25T19:14:07.962946300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # Used for debugging; CUDA related errors shown immediately.\n",
    "\n",
    "# Seed everything for reproducible results\n",
    "seed = 2024\n",
    "np.random.seed(seed)\n",
    "np.random.default_rng(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:14:08.367167800Z",
     "start_time": "2024-05-25T19:14:08.349354300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model_TrainTest:\n",
    "    def __init__(self, seed, device, hyperparams, agent='dqn'):\n",
    "        self.agent=agent\n",
    "\n",
    "        # Define RL Hyperparameters\n",
    "        self.train_mode             = hyperparams[\"train_mode\"]\n",
    "        self.RL_load_path           = hyperparams[\"RL_load_path\"]\n",
    "        self.save_path              = hyperparams[\"save_path\"]\n",
    "        self.save_interval          = hyperparams[\"save_interval\"]\n",
    "        \n",
    "        self.clip_grad_norm         = hyperparams[\"clip_grad_norm\"]\n",
    "        self.learning_rate          = hyperparams[\"learning_rate\"]\n",
    "        self.discount_factor        = hyperparams[\"discount_factor\"]\n",
    "        self.batch_size             = hyperparams[\"batch_size\"]\n",
    "        self.update_frequency       = hyperparams[\"update_frequency\"]\n",
    "        self.max_episodes           = hyperparams[\"max_episodes\"]\n",
    "        self.max_episode_length     = hyperparams[\"max_episode_length\"]\n",
    "        self.max_steps              = hyperparams[\"max_steps\"]\n",
    "        self.render                 = hyperparams[\"render\"]\n",
    "        \n",
    "        self.epsilon_max            = hyperparams[\"epsilon_max\"]\n",
    "        self.epsilon_min            = hyperparams[\"epsilon_min\"]\n",
    "        self.epsilon_decay          = hyperparams[\"epsilon_decay\"]\n",
    "        \n",
    "        self.memory_capacity        = hyperparams[\"memory_capacity\"]\n",
    "        \n",
    "        self.num_states             = hyperparams[\"num_states\"]\n",
    "        # self.map_size               = hyperparams[\"map_size\"]\n",
    "        self.map                    = hyperparams[\"map\"]\n",
    "        self.render_fps             = hyperparams[\"render_fps\"]\n",
    "        \n",
    "        self.n_bins                 = hyperparams[\"n_bins\"]\n",
    "        \n",
    "        self.env = make_frozenlake(self.map)\n",
    "        self.env.metadata['render_fps'] = self.render_fps # For max frame rate make it 0\n",
    "        \n",
    "        # Define the agent class\n",
    "        if agent == 'dqn':\n",
    "            self.agent = DQN_Agent(env            = self.env, \n",
    "                                seed              = seed,\n",
    "                                device            = device,  \n",
    "                                epsilon_max       = self.epsilon_max, \n",
    "                                epsilon_min       = self.epsilon_min, \n",
    "                                epsilon_decay     = self.epsilon_decay,\n",
    "                                clip_grad_norm    = self.clip_grad_norm,\n",
    "                                learning_rate     = self.learning_rate,\n",
    "                                discount          = self.discount_factor,\n",
    "                                memory_capacity   = self.memory_capacity)\n",
    "        if agent == 'ucb':\n",
    "            self.agent = UCB_Agent(env            = self.env, \n",
    "                                seed              = seed,\n",
    "                                device            = device,\n",
    "                                epsilon_max       = self.epsilon_max, \n",
    "                                epsilon_min       = self.epsilon_min, \n",
    "                                epsilon_decay     = self.epsilon_decay,\n",
    "                                clip_grad_norm    = self.clip_grad_norm,\n",
    "                                learning_rate     = self.learning_rate,\n",
    "                                discount          = self.discount_factor,\n",
    "                                memory_capacity   = self.memory_capacity)\n",
    "        if agent == 'ids':\n",
    "            self.agent = IDS_Agent(   env               = self.env, \n",
    "                                seed              = seed,\n",
    "                                device            = device,\n",
    "                                epsilon_max       = self.epsilon_max, \n",
    "                                epsilon_min       = self.epsilon_min, \n",
    "                                epsilon_decay     = self.epsilon_decay,\n",
    "                                clip_grad_norm    = self.clip_grad_norm,\n",
    "                                learning_rate     = self.learning_rate,\n",
    "                                discount          = self.discount_factor,\n",
    "                                memory_capacity   = self.memory_capacity,\n",
    "                                num_ensembles     = 10)\n",
    "    \n",
    "        self.loss_bins = []\n",
    "                \n",
    "        \n",
    "    def state_preprocess(self, state:int, num_states:int):\n",
    "        \"\"\"\n",
    "        Convert an state to a tensor and basically it encodes the state into \n",
    "        an onehot vector. For example, the return can be something like tensor([0,0,1,0,0]) \n",
    "        which could mean agent is at state 2 from total of 5 states.\n",
    "\n",
    "        \"\"\"\n",
    "        onehot_vector = torch.zeros(num_states, dtype=torch.float32, device=device)\n",
    "        onehot_vector[state] = 1\n",
    "        return onehot_vector\n",
    "    \n",
    "    \n",
    "    def train(self): \n",
    "        \"\"\"                \n",
    "        Reinforcement learning training loop.\n",
    "        \"\"\"        \n",
    "        total_steps = 0\n",
    "        episode = 0\n",
    "        self.reward_history = []\n",
    "        self.episode_length_history = []\n",
    "        self.environment_steps_history = []\n",
    "        \n",
    "        # Training loop over episodes\n",
    "        while total_steps < self.max_steps and episode < self.max_episodes:\n",
    "            state = self.env.reset()\n",
    "            state = self.state_preprocess(state, num_states=self.num_states)\n",
    "            done = False\n",
    "            truncation = False\n",
    "            step_size = 0\n",
    "            episode_reward = 0\n",
    "                                                \n",
    "            while not done and not truncation:\n",
    "                action = self.agent.select_action(state)\n",
    "                next_state, reward, done, truncation = self.env.step(action)\n",
    "                next_state = self.state_preprocess(next_state, num_states=self.num_states)\n",
    "                \n",
    "                self.agent.replay_memory.store(state, action, next_state, reward, done) \n",
    "                \n",
    "                # if len(self.agent.replay_memory) > self.batch_size and sum(self.reward_history) > 0: This was the original but since we have a negative reward so the sum doesn't work the same way anymore\n",
    "                if len(self.agent.replay_memory) > self.batch_size:\n",
    "                    self.agent.learn(self.batch_size, (done or truncation))\n",
    "                \n",
    "                    # Update target-network weights\n",
    "                    if total_steps % self.update_frequency == 0:\n",
    "                        self.agent.hard_update()\n",
    "                    \n",
    "                # Penalize if the episode is truncated (BAD PRACTICE, NON MARKOVIAN)\n",
    "                if not truncation and step_size >= self.max_episode_length:\n",
    "                    truncation = True \n",
    "                    reward = -10\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step_size +=1\n",
    "                            \n",
    "            # Appends for tracking history\n",
    "            total_steps += step_size\n",
    "            self.reward_history.append(episode_reward) # episode reward    \n",
    "            self.episode_length_history.append(step_size) # episode length\n",
    "            self.environment_steps_history.append(total_steps) # total steps\n",
    "            episode += 1\n",
    "                                                                           \n",
    "            # Decay epsilon at the end of each episode\n",
    "            self.agent.update_epsilon()\n",
    "            \n",
    "            result = (f\"Episode: {episode}, \"\n",
    "                      f\"Total Steps: {total_steps}, \"\n",
    "                      f\"Raw Reward: {episode_reward:.2f}, \")\n",
    "            # print(result)\n",
    "                                                                    \n",
    "\n",
    "    def test(self, max_episodes):  \n",
    "        \"\"\"                \n",
    "        Reinforcement learning policy evaluation.\n",
    "        \"\"\"\n",
    "           \n",
    "        # Load the weights of the test_network\n",
    "        self.agent.main_network.load_state_dict(torch.load(self.RL_load_path))\n",
    "        self.agent.main_network.eval()\n",
    "        \n",
    "        # Testing loop over episodes\n",
    "        for episode in range(1, max_episodes+1):         \n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            truncation = False\n",
    "            step_size = 0\n",
    "            episode_reward = 0\n",
    "                                                           \n",
    "            while not done and not truncation:\n",
    "                state = self.state_preprocess(state, num_states=self.num_states)\n",
    "                action = self.agent.select_action(state)\n",
    "                next_state, reward, done, truncation, _ = self.env.step(action)\n",
    "                                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step_size += 1\n",
    "                                                                                                                       \n",
    "            # Print log            \n",
    "            result = (f\"Episode: {episode}, \"\n",
    "                      f\"Steps: {step_size:}, \"\n",
    "                      f\"Reward: {episode_reward:.2f}, \")\n",
    "            print(result)\n",
    "            \n",
    "        pygame.quit() # close the rendering window\n",
    "    \n",
    "    def plot_training(self):\n",
    "        plotting.plot_binned_line_with_std([self.environment_steps_history], [self.reward_history], self.n_bins, y_label=\"Reward\", title=f\"Reward over time ({self.agent})\", plot_individuals=False)\n",
    "        plotting.plot_binned_line_with_std([self.environment_steps_history], [self.episode_length_history], self.n_bins, y_label=\"Episode Length\", title=f\"Episode Length over time ({self.agent})\", plot_individuals=False)\n",
    "        print(len(self.environment_steps_history[3:]), len(self.agent.loss_history))\n",
    "        plotting.plot_binned_line_with_std([self.environment_steps_history[3:]], [self.agent.loss_history], self.n_bins, y_label=\"Loss\", title=f\"Loss over time ({self.agent})\", plot_individuals=False)\n",
    "\n",
    "    def get_plotting_data(self):\n",
    "        padded_loss_history = self.agent.loss_history\n",
    "        while len(padded_loss_history) < len(self.environment_steps_history):\n",
    "            padded_loss_history = [padded_loss_history[0]] + padded_loss_history\n",
    "        return self.environment_steps_history, self.reward_history, self.episode_length_history, padded_loss_history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:14:08.378042400Z",
     "start_time": "2024-05-25T19:14:08.365173Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_aleatoric_uncertainty_3x3 = [\n",
    "    ['S', '.', '.'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', '.', 'G']\n",
    "]\n",
    "\n",
    "safe_3x3 = [\n",
    "    ['S', 'X', '.'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', '.', 'G']\n",
    "]\n",
    "\n",
    "long_safe_4x3 = [\n",
    "    ['S', 'X', 'G'],\n",
    "    ['.', 'X', '.'],\n",
    "    ['.', 'X', '.'],\n",
    "    ['.', '.', '.']\n",
    "]\n",
    "\n",
    "short_unsafe_long_safe_4x3 = [\n",
    "    ['S', 'X', 'G'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', 'X', '.'],\n",
    "    ['.', '.', '.']\n",
    "]\n",
    "\n",
    "unsafe_path_safe_area_3x4 = [\n",
    "    ['S', '.', 'X', '.'],\n",
    "    ['.', '.', '.', '.'],\n",
    "    ['.', '.', 'X', 'G']\n",
    "]\n",
    "\n",
    "random_test_4x4 = [\n",
    "    ['S', '.', 'X'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', 'X', '.'],\n",
    "    ['.', '.', 'G']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:33:27.499836400Z",
     "start_time": "2024-05-25T19:14:08.375050400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "train_mode = True\n",
    "agent = 'ucb'               # 'dqn', 'ucb' or 'ids'\n",
    "\n",
    "render = not train_mode\n",
    "# map_size = 4 # 4x4 or 8x8 (outdated)\n",
    "map_mame = \"long_safe_4x3\"\n",
    "RL_hyperparams = {\n",
    "    \"train_mode\"            : train_mode,\n",
    "    \"RL_load_path\"          : f'./level_stats/{map_mame}/final_weights' + '_' + '3000' + '.pth',\n",
    "    \"save_path\"             : f'./level_stats/{map_mame}/final_weights',\n",
    "    \"save_interval\"         : 500,\n",
    "    \n",
    "    \"clip_grad_norm\"        : 3,\n",
    "    \"learning_rate\"         : 6e-4,\n",
    "    \"discount_factor\"       : 0.93,\n",
    "    \"batch_size\"            : 32,\n",
    "    \"update_frequency\"      : 10,\n",
    "    \"max_episodes\"          : 2000           if train_mode else 5,\n",
    "    \"max_steps\"             : 50000,\n",
    "    \"max_episode_length\"    : 1000,\n",
    "    \"render\"                : render,\n",
    "    \n",
    "    \"epsilon_max\"           : 0.999         if train_mode else -1,\n",
    "    \"epsilon_min\"           : 0.01,\n",
    "    \"epsilon_decay\"         : 0.999,\n",
    "    \n",
    "    \"memory_capacity\"       : 4_000        if train_mode else 0,\n",
    "        \n",
    "    # \"map_size\"              : map_size,\n",
    "    \"num_states\"            : len(safe_3x3) * len(safe_3x3[0]),    # 3 rows in your example\n",
    "    \"map\"                   : safe_3x3,\n",
    "    \"render_fps\"            : 6,\n",
    "    \"n_bins\"                : 100\n",
    "    }\n",
    "\n",
    "\n",
    "# Run\n",
    "DRL = Model_TrainTest(seed, device, RL_hyperparams, agent=agent) # Define the instance\n",
    "# Train\n",
    "if train_mode:\n",
    "    environment_steps_histories, reward_histories, episode_length_histories, loss_histories = [], [], [], []\n",
    "    for i in range(10):\n",
    "        print(i)\n",
    "        DRL.train()\n",
    "\n",
    "        # Save all the data for plotting (plotted in next cell)\n",
    "        environment_steps_history, reward_history, episode_length_history, loss_history = DRL.get_plotting_data()\n",
    "        environment_steps_histories.append(environment_steps_history)\n",
    "        reward_histories.append(reward_history)\n",
    "        episode_length_histories.append(episode_length_history)\n",
    "        loss_histories.append(loss_history)\n",
    "else:\n",
    "    # Test\n",
    "    DRL.test(max_episodes = RL_hyperparams['max_episodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data (only works in train mode)\n",
    "n_bins = 100\n",
    "plotting.plot_binned_line_with_std(environment_steps_histories, reward_histories, n_bins, y_label=\"Reward\", title=\"Reward over time\", plot_individuals=True)\n",
    "plotting.plot_binned_line_with_std(environment_steps_histories, episode_length_histories, n_bins, y_label=\"Episode Length\", title=\"Episode Length over time\", plot_individuals=True)\n",
    "plotting.plot_binned_line_with_std(environment_steps_histories, loss_histories, n_bins, y_label=\"Loss\", title=\"Loss over time\", plot_individuals=True)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
