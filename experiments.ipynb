{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import plotting\n",
    "from utils.frozenlake import make_frozenlake\n",
    "from algorithms.DQN_Agent import DQN_Agent\n",
    "from algorithms.IDS import IDS_Agent\n",
    "from algorithms.UCB import UCB_Agent\n",
    "from algorithms.NEW import NewAlg_Agent"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # Used for debugging; CUDA related errors shown immediately.\n",
    "\n",
    "# Seed everything for reproducible results\n",
    "seed = 2024\n",
    "np.random.seed(seed)\n",
    "np.random.default_rng(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    "
   ],
   "id": "7cac2aa2df694066",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Model_TrainTest:\n",
    "    def __init__(self, seed, device, hyperparams, agent_type='dqn'):\n",
    "        # Define RL Hyperparameters\n",
    "        self.train_mode             = hyperparams[\"train_mode\"]\n",
    "        self.save_interval          = hyperparams[\"save_interval\"]\n",
    "        \n",
    "        self.clip_grad_norm         = hyperparams[\"clip_grad_norm\"]\n",
    "        self.learning_rate          = hyperparams[\"learning_rate\"]\n",
    "        self.discount_factor        = hyperparams[\"discount_factor\"]\n",
    "        self.batch_size             = hyperparams[\"batch_size\"]\n",
    "        self.update_frequency       = hyperparams[\"update_frequency\"]\n",
    "        self.max_episodes           = hyperparams[\"max_episodes\"]\n",
    "        self.max_episode_length     = hyperparams[\"max_episode_length\"]\n",
    "        self.max_steps              = hyperparams[\"max_steps\"]\n",
    "        self.render                 = hyperparams[\"render\"]\n",
    "        \n",
    "        self.epsilon_max            = hyperparams[\"epsilon_max\"]\n",
    "        self.epsilon_min            = hyperparams[\"epsilon_min\"]\n",
    "        self.epsilon_decay          = hyperparams[\"epsilon_decay\"]\n",
    "        \n",
    "        self.memory_capacity        = hyperparams[\"memory_capacity\"]\n",
    "        \n",
    "        self.num_states             = hyperparams[\"num_states\"]\n",
    "        # self.map_size               = hyperparams[\"map_size\"]\n",
    "        self.map                    = hyperparams[\"map\"]\n",
    "        self.render_fps             = hyperparams[\"render_fps\"]\n",
    "        \n",
    "        self.env = make_frozenlake(self.map)\n",
    "        self.env.metadata['render_fps'] = self.render_fps # For max frame rate make it 0\n",
    "        self.agent_type = agent_type\n",
    "        # Define the agent class\n",
    "        self.agent = None\n",
    "        self.reset_agent()\n",
    "        self.agent_type = agent_type\n",
    "        self.state_history = []\n",
    "        self.loss_bins = []\n",
    "\n",
    "    def state_preprocess(self, state:int, num_states:int):\n",
    "        \"\"\"\n",
    "        Convert an state to a tensor and basically it encodes the state into\n",
    "        an onehot vector. For example, the return can be something like tensor([0,0,1,0,0])\n",
    "        which could mean agent is at state 2 from total of 5 states.\n",
    "\n",
    "        \"\"\"\n",
    "        onehot_vector = torch.zeros(num_states, dtype=torch.float32, device=device)\n",
    "        onehot_vector[state] = 1\n",
    "        return onehot_vector\n",
    "    \n",
    "    \n",
    "    def reset_agent(self):\n",
    "        if self.agent_type == 'dqn':\n",
    "            self.agent = DQN_Agent(env            = self.env,\n",
    "                                seed              = seed,\n",
    "                                device            = device,\n",
    "                                epsilon_max       = self.epsilon_max,\n",
    "                                epsilon_min       = self.epsilon_min,\n",
    "                                epsilon_decay     = self.epsilon_decay,\n",
    "                                clip_grad_norm    = self.clip_grad_norm,\n",
    "                                learning_rate     = self.learning_rate,\n",
    "                                discount          = self.discount_factor,\n",
    "                                memory_capacity   = self.memory_capacity)\n",
    "        if self.agent_type == 'ucb':\n",
    "            self.agent = UCB_Agent(env            = self.env,\n",
    "                                seed              = seed,\n",
    "                                device            = device,\n",
    "                                epsilon_max       = self.epsilon_max,\n",
    "                                epsilon_min       = self.epsilon_min,\n",
    "                                epsilon_decay     = self.epsilon_decay,\n",
    "                                clip_grad_norm    = self.clip_grad_norm,\n",
    "                                learning_rate     = self.learning_rate,\n",
    "                                discount          = self.discount_factor,\n",
    "                                memory_capacity   = self.memory_capacity)\n",
    "        if self.agent_type == 'ids':\n",
    "            self.agent = IDS_Agent(   env               = self.env,\n",
    "                                seed              = seed,\n",
    "                                device            = device,\n",
    "                                epsilon_max       = self.epsilon_max,\n",
    "                                epsilon_min       = self.epsilon_min,\n",
    "                                epsilon_decay     = self.epsilon_decay,\n",
    "                                clip_grad_norm    = self.clip_grad_norm,\n",
    "                                learning_rate     = self.learning_rate,\n",
    "                                discount          = self.discount_factor,\n",
    "                                memory_capacity   = self.memory_capacity,\n",
    "                                num_ensembles     = 10)\n",
    "        if self.agent_type == 'new':\n",
    "            self.agent = NewAlg_Agent(   env               = self.env,\n",
    "                                seed              = seed,\n",
    "                                device            = device,\n",
    "                                epsilon_max       = self.epsilon_max,\n",
    "                                epsilon_min       = self.epsilon_min,\n",
    "                                epsilon_decay     = self.epsilon_decay,\n",
    "                                clip_grad_norm    = self.clip_grad_norm,\n",
    "                                learning_rate     = self.learning_rate,\n",
    "                                discount          = self.discount_factor,\n",
    "                                memory_capacity   = self.memory_capacity,\n",
    "                                num_ensembles     = 10)\n",
    "    \n",
    "    def run(self, training, agent=None):\n",
    "        \"\"\"                \n",
    "        Reinforcement learning training loop.\n",
    "        \"\"\"        \n",
    "        total_steps = 0\n",
    "        episode = 0\n",
    "        self.reward_history = []\n",
    "        self.episode_length_history = []\n",
    "        self.environment_steps_history = []\n",
    "        if not training:\n",
    "            self.agent = agent\n",
    "        else:\n",
    "            self.reset_agent()\n",
    "\n",
    "        # Training loop over episodes\n",
    "        while total_steps < self.max_steps:\n",
    "            state = self.env.reset()\n",
    "            self.state_history.append(state)\n",
    "            state = self.state_preprocess(state, num_states=self.num_states)\n",
    "            done = False\n",
    "            truncation = False\n",
    "            step_size = 0\n",
    "            episode_reward = 0\n",
    "                                                \n",
    "            while not done and not truncation:\n",
    "                action = self.agent.select_action(state)\n",
    "                next_state, reward, done, truncation = self.env.step(action)\n",
    "                self.state_history.append(next_state)\n",
    "                next_state = self.state_preprocess(next_state, num_states=self.num_states)\n",
    "\n",
    "                if(training):\n",
    "                    self.agent.replay_memory.store(state, action, next_state, reward, done)\n",
    "\n",
    "                    # if len(self.agent.replay_memory) > self.batch_size and sum(self.reward_history) > 0: This was the original but since we have a negative reward so the sum doesn't work the same way anymore\n",
    "                    if len(self.agent.replay_memory) > self.batch_size:\n",
    "                        self.agent.learn(self.batch_size, (done or truncation))\n",
    "\n",
    "                        # Update target-network weights\n",
    "                        if (total_steps+step_size) % self.update_frequency == 0:\n",
    "                            self.agent.hard_update()\n",
    "\n",
    "                    if not truncation and step_size >= self.max_episode_length:\n",
    "                        truncation = True\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step_size +=1\n",
    "                            \n",
    "            # Appends for tracking history\n",
    "            total_steps += step_size\n",
    "            self.reward_history.append(episode_reward) # episode reward    \n",
    "            self.episode_length_history.append(step_size) # episode length\n",
    "            self.environment_steps_history.append(total_steps) # total steps\n",
    "            episode += 1\n",
    "                                                                           \n",
    "            # Decay epsilon at the end of each episode\n",
    "            self.agent.update_epsilon()\n",
    "            \n",
    "            result = (f\"Episode: {episode}, \"\n",
    "                      f\"Total Steps: {total_steps}, \"\n",
    "                      f\"Raw Reward: {episode_reward:.2f}, \")\n",
    "            print(result)\n",
    "        return self.agent                                                \n",
    "    \n",
    "    def plot_training(self):\n",
    "        plotting.plot_binned_line_with_std([self.environment_steps_history], [self.reward_history], self.n_bins, y_label=\"Reward\", title=f\"Reward over time ({self.agent_type})\", plot_individuals=False)\n",
    "        plotting.plot_binned_line_with_std([self.environment_steps_history], [self.episode_length_history], self.n_bins, y_label=\"Episode Length\", title=f\"Episode Length over time ({self.agent_type})\", plot_individuals=False)\n",
    "        print(len(self.environment_steps_history[3:]), len(self.agent.loss_history))\n",
    "        plotting.plot_binned_line_with_std([self.environment_steps_history[3:]], [self.agent.loss_history], self.n_bins, y_label=\"Loss\", title=f\"Loss over time ({self.agent_type})\", plot_individuals=False)\n",
    "\n",
    "    def get_plotting_data(self):\n",
    "        padded_loss_history = np.array(self.agent.loss_history)\n",
    "        while len(padded_loss_history) < len(self.environment_steps_history):\n",
    "            padded_loss_history = np.concatenate(([padded_loss_history[0]],padded_loss_history))\n",
    "        return self.environment_steps_history, self.reward_history, self.episode_length_history, padded_loss_history.tolist(), self.state_history"
   ],
   "id": "4a160508f923e372",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "no_aleatoric_uncertainty_3x3 = [\n",
    "    ['S', '.', '.'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', '.', 'G']\n",
    "]\n",
    "\n",
    "safe_3x3 = [\n",
    "    ['S', 'X', '.'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', '.', 'G']\n",
    "]\n",
    "\n",
    "long_safe_4x3 = [\n",
    "    ['S', 'X', 'G'],\n",
    "    ['.', 'X', '.'],\n",
    "    ['.', 'X', '.'],\n",
    "    ['.', '.', '.']\n",
    "]\n",
    "\n",
    "short_unsafe_long_safe_4x3 = [\n",
    "    ['S', 'X', 'G'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', 'X', '.'],\n",
    "    ['.', '.', '.']\n",
    "]\n",
    "\n",
    "unsafe_path_safe_area_3x4 = [\n",
    "    ['S', '.', 'X', '.'],\n",
    "    ['.', '.', '.', '.'],\n",
    "    ['.', '.', 'X', 'G']\n",
    "]\n",
    "\n",
    "random_test_4x4 = [\n",
    "    ['S', '.', 'X'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', 'X', '.'],\n",
    "    ['.', '.', 'G']\n",
    "]"
   ],
   "id": "b654ab9ed14d491b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_results(env_steps_histories, eps_length_histories, map_state_histories, agent_reward_histories, agent_loss_histories, state_map, agents, n_bins):\n",
    "    if train_mode:\n",
    "        plotting.plot_binned_line_with_std(env_steps_histories, agent_reward_histories, n_bins, y_label=\"Reward\", title=\"Reward over time\", plot_individuals=True)\n",
    "        plotting.plot_binned_line_with_std(env_steps_histories, eps_length_histories, n_bins, y_label=\"Episode Length\", title=\"Episode Length over time\", plot_individuals=True)\n",
    "        plotting.plot_binned_line_with_std(env_steps_histories, agent_loss_histories, n_bins, y_label=\"Loss\", title=\"Loss over time\", plot_individuals=True)\n",
    "    else:\n",
    "        for i in range(len(map_state_histories)):\n",
    "            state_table = np.zeros_like(state_map)\n",
    "            print(agents[i].get_best_actions(state_table, device))\n",
    "            plotting.plot_grid_statespace(map_state_histories[i], agents[i].get_best_actions(state_table, device), state_map)\n",
    "        plotting.plot_barchart_rewards(agent_reward_histories, y_label=\"Reward Counts\", title=\"Bar Chart of Unique Values and Their Corresponding Reward Counts\")\n",
    "        plotting.plot_barchart_episode_length(eps_length_histories, n_bins, y_label='Amount of Finished Runs', title=\"Bar Chart of Unique Values and Their Corresponding Reward Counts\")\n",
    "        \n",
    "def run_model(agents):\n",
    "    DRL = Model_TrainTest(seed, device, RL_hyperparams, agent_type=agent_type) # Define the instance\n",
    "    environment_steps_histories, reward_histories, episode_length_histories, loss_histories, state_histories = [], [], [], [], []\n",
    "    if train_mode:\n",
    "        # 20 runs per agent\n",
    "        for i in range(20):\n",
    "            print(i)\n",
    "            trained_agent = DRL.run(train_mode, None)\n",
    "            agents.append(trained_agent)\n",
    "    \n",
    "            # Save all the data for plotting (plotted in next cell)\n",
    "            environment_steps_history, reward_history, episode_length_history, loss_history, state_history = DRL.get_plotting_data()\n",
    "            environment_steps_histories.append(environment_steps_history)\n",
    "            reward_histories.append(reward_history)\n",
    "            episode_length_histories.append(episode_length_history)\n",
    "            loss_histories.append(loss_history)\n",
    "            state_histories.append(state_history)\n",
    "    else:\n",
    "        for agent in agents:\n",
    "            DRL.run(train_mode, agent)\n",
    "            \n",
    "            # Save all the data for plotting (plotted in next cell)\n",
    "            environment_steps_history, reward_history, episode_length_history, loss_history, state_history = DRL.get_plotting_data()\n",
    "            environment_steps_histories.append(environment_steps_history)\n",
    "            reward_histories.append(reward_history)\n",
    "            episode_length_histories.append(episode_length_history)\n",
    "            loss_histories.append(loss_history)\n",
    "            state_histories.append(state_history)\n",
    "    return environment_steps_histories, reward_histories, episode_length_histories, loss_histories, state_histories, agents"
   ],
   "id": "968cc02618419015",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parameters:\n",
    "train_mode = True\n",
    "agent_type = 'new'               # 'dqn', 'ucb', 'ids', 'new'\n",
    "\n",
    "render = not train_mode\n",
    "\n",
    "state_map = short_unsafe_long_safe_4x3\n",
    "RL_hyperparams = {\n",
    "    \"train_mode\"            : train_mode,\n",
    "\n",
    "    \"save_interval\"         : 500,\n",
    "    \n",
    "    \"clip_grad_norm\"        : 3,\n",
    "    \"learning_rate\"         : 6e-4,\n",
    "    \"discount_factor\"       : 0.93,\n",
    "    \"batch_size\"            : 32,\n",
    "    \"update_frequency\"      : 10,\n",
    "    \"max_episodes\"          : 100000000           if train_mode else 200,\n",
    "    \"max_steps\"             : 200000,               \n",
    "    \"max_episode_length\"    : 1000,\n",
    "    \"render\"                : render,\n",
    "    \n",
    "    \"epsilon_max\"           : 0.999         if train_mode else 0,\n",
    "    \"epsilon_min\"           : 0.01,\n",
    "    \"epsilon_decay\"         : 0.998,\n",
    "    \n",
    "    \"memory_capacity\"       : 4_000        if train_mode else 0,\n",
    "\n",
    "    \"num_states\"            : len(state_map) * len(state_map[0]),   \n",
    "    \"map\"                   : state_map,\n",
    "    \"render_fps\"            : 6,\n",
    "    }\n",
    "\n",
    "n_bins = 50\n",
    "environment_steps_histories, reward_histories, episode_length_histories, loss_histories, state_histories, agent_histories = run_model([])"
   ],
   "id": "c629f551a1ccd12b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_results(environment_steps_histories, episode_length_histories, state_histories, reward_histories, loss_histories, state_map, agent_histories, n_bins)",
   "id": "552ed11ff9dbc38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_mode = False\n",
    "RL_hyperparams[train_mode] = train_mode\n",
    "\n",
    "environment_steps_histories, reward_histories, episode_length_histories, loss_histories, state_histories, agent_histories = run_model(agent_histories)\n",
    "\n",
    "plot_results(environment_steps_histories, episode_length_histories, state_histories, reward_histories, loss_histories, state_map, agent_histories, n_bins)"
   ],
   "id": "790377c35f911a1b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
