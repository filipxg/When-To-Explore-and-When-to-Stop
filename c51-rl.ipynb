{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pygame\n!pip install tyro","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport os\nimport random\nimport time\n\nfrom gym import spaces\nimport gymnasium as gym\nimport matplotlib.colors as mcolors\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pygame\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport tyro\nfrom collections import deque\nfrom collections import Counter\nfrom dataclasses import dataclass\nfrom stable_baselines3.common.buffers import ReplayBuffer\nfrom torch.utils.tensorboard import SummaryWriter\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inputs: xss - n lists of x values, where n is the amount of times the experiments \ndef plot_binned_line_with_std(xss, yss, n_bins, y_label = \"\", title = \"\", plot_individuals = False):\n    assert len(xss) == len(yss)\n\n    mx = np.max([np.max(xs) for xs in xss])\n\n    bin_size = (mx+1) / n_bins\n\n    binned = []\n\n    # What to plot on the x axis\n    bins_x = np.linspace(0, mx, n_bins)\n    \n    for i in range(len(xss)):\n        xs = xss[i]\n        ys = yss[i]\n        bins = [[] for _ in range(n_bins)]\n        for j in range(len(xs)):\n            bin_index = int(xs[j] / bin_size)\n            bins[bin_index].append(ys[j])\n        binned.append(bins)\n\n    avgs = [[np.mean(bin) for bin in binned[i]] for i in range(len(xss))]\n    avg = np.mean(avgs, axis=0)\n    std = np.std(avgs, axis=0)\n\n    plt.figure(figsize=(10, 6))\n\n    if plot_individuals:\n        c = 0\n        for a in avgs:\n            colours = ['red', 'green', 'blue', 'orange', 'purple', 'brown', 'pink', 'gray', 'cyan', 'magenta']\n            plt.plot(bins_x, a, linestyle=':', linewidth=1, alpha=0.7, color=colours[c])\n            c += 1\n\n    plt.plot(bins_x, avg, label='Average', color='blue')\n    plt.fill_between(bins_x, avg - std, avg + std, color='blue', alpha=0.2, label='Standard Deviation')\n\n    plt.xlabel('Environment steps')\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef plot_grid_heatmap(uncertainties, best_actions, colour_scheme = \"ryg\"):\n    # Possible colour schemes: \"ryg\", \"light_ryg\", \"hot/cold\"\n    height, width, _ = uncertainties.shape\n    heatmap_values = np.zeros((height, width))\n    for x in range(width):\n        for y in range(height):\n            action = best_actions[y, x]\n            heatmap_values[y, x] = uncertainties[y, x, action]\n\n    cmap = {\n        \"ryg\": mcolors.LinearSegmentedColormap.from_list(\"stoplight\", [(0, \"green\"), (0.5, \"yellow\"), (1, \"red\")]),\n        \"light ryg\": mcolors.LinearSegmentedColormap.from_list(\"stoplight\", [(0, \"#66cdaa\"), (0.5, \"#fffacd\"), (1, \"#ff9999\")]),\n        \"hot/cold\": \"coolwarm\",\n        }[colour_scheme]\n\n    plt.figure(figsize=(10, 8))\n    plt.imshow(heatmap_values, cmap=cmap, origin='upper', interpolation='nearest')\n    plt.colorbar(label='Uncertainty')\n    plt.title('Uncertainties for each best action per cell')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n\n    for x in range(width):\n        for y in range(height):\n            plt.text(x, y, f'{heatmap_values[y, x]:.1f}', ha='center', va='center', color='black')\n\n    plt.show()\n\n\ndef plot_barchart_rewards(reward_histories, y_label, title):\n    n_agents = len(reward_histories)\n    all_rewards = [reward for rewards in reward_histories for reward in rewards]\n\n    # Separate the unique values and their corresponding counts\n    unique_values, reward_counts = np.unique(np.array(all_rewards), return_counts=True)\n\n    # Calculate the width for the bars to be adjacent\n    width = np.min(np.diff(unique_values)) / n_agents if len(unique_values) > 1 else 1.0\n\n    plt.figure()\n    plt.bar(unique_values, reward_counts/n_agents, width=width, align='center')\n    plt.xlabel('Unique Values')\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.xticks(unique_values)  # Ensure each unique value has a tick\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_barchart_episode_length(episode_length_histories, n_bins, y_label, title):\n    # Determine the number of agents and the maximum episode length\n    n_agents = len(episode_length_histories)\n    max_length = max(max(lengths) for lengths in episode_length_histories)\n\n    # Flatten the 2D list into a 1D list of all episode lengths\n    all_lengths = [length for agent_lengths in episode_length_histories for length in agent_lengths]\n\n    # Initialize the figure and axis\n    fig, ax = plt.subplots()\n\n    # Create the histogram data with the specified number of bins\n    counts, bin_edges = np.histogram(all_lengths, bins=n_bins)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    # Divide the counts by the number of agents to get the average occurrence\n    average_counts = counts / n_agents\n\n    # Plot the bar chart\n    plt.xlabel('Episode Length')\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.bar(bin_centers, average_counts, width=(bin_edges[1] - bin_edges[0]) - 0.1, align='center')\n\n\ndef qtable_directions_map(qtable, map):\n    \"\"\"Get the best learned action & map it to arrows.\"\"\"\n    directions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n    qtable = qtable.flatten()\n    for idx, val in enumerate(qtable):\n        qtable[int(idx)] = directions[int(val)]\n    qtable_directions = qtable.reshape(len(map), len(map[0]))\n    return qtable_directions\n\ndef plot_grid_statespace(state_history, optimal_moves, state_map):\n    qtable_directions = qtable_directions_map(optimal_moves, state_map)\n    for y in range(len(qtable_directions)):\n        for x in range(len(qtable_directions[0])):\n            if state_map[y][x] == 'X' or state_map[y][x] == 'G':\n                qtable_directions[y, x] = ''\n    state_counts = np.bincount(state_history)\n\n    # Step 2: Normalize the visit counts\n    max_count = np.max(state_counts)\n    normalized_counts = state_counts / max_count\n    reshaped_counts = np.reshape(normalized_counts, (len(state_map), len(state_map[0])))\n    plt.figure()\n    sns.heatmap(\n        reshaped_counts,\n        annot=qtable_directions,\n        fmt=\"\",\n        cmap=sns.color_palette(\"Blues\", as_cmap=True),\n        linewidths=0.7,\n        linecolor=\"black\",\n        xticklabels=[],\n        yticklabels=[],\n        annot_kws={\"fontsize\": \"xx-large\"},\n    ).set(title=\"Learned Q-values\\nArrows represent best action\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FrozenLakeEnv(gym.Env):\n    def __init__(self, grid):\n        super(FrozenLakeEnv, self).__init__()\n        self.grid = grid\n        self.grid_height = len(grid)\n        self.grid_width = len(grid[0])\n        [self.start_y], [self.start_x] = np.where(np.array(grid) == 'S')\n        self.slip_probability = 1/3 # Probability to slip to one side (so the chance of slipping in any direction is 2 times this value)\n        assert self.slip_probability <= 1/2\n        self.action_space = spaces.Discrete(4)  # Left, Down, Right, Up\n        self.observation_space = spaces.Discrete(self.grid_height * self.grid_width)\n\n        self.state_action_count = {}\n        for x in range(self.grid_width):\n            for y in range(self.grid_height):\n                self.state_action_count[(x, y)] = {0: 0, 1: 0, 2: 0, 3: 0}\n\n    def reset(self):\n        # Top left corner is 0, 0\n        self.state = (self.start_x, self.start_y)\n        return self.to_observation(self.state)\n    \n    def step(self, action):\n        self.state_action_count[self.state][action] += 1\n        x, y = self.state\n\n        # Define possible actions for each chosen direction\n        # Make sure the first action in the array is the action itself (no slip)\n        possible_actions = {\n                0: [0, 3, 1],  # Left\n                1: [1, 0, 2],  # Down\n                2: [2, 1, 3],  # Right\n                3: [3, 2, 0]   # Up\n            }            \n\n        # Choose a random action from the possible actions according to self.slip_probability\n        p = self.slip_probability\n        action = np.random.choice(possible_actions[action], p=[1-2*p, p, p])\n        # print(\"Actual action\", [\"left\", \"down\", \"right\", \"up\"][action])\n\n        # Move in the chosen direction if its within bounds\n        if action == 0 and x > 0:\n            x -= 1\n        elif action == 1 and y < self.grid_height - 1:\n            y += 1\n        elif action == 2 and x < self.grid_width - 1:\n            x += 1\n        elif action == 3 and y > 0:\n            y -= 1\n\n        self.state = (x, y)\n        reward = 0\n        done = False\n\n        # Check state of the cell\n        if self.grid[y][x] == 'X':\n            reward = -5\n            done = True\n        elif self.grid[y][x] == 'G':\n            reward = 10\n            done = True\n            \n#         print(f\"State: {self.state}, Action: {action}, Reward: {reward}, Done: {done}\")\n\n        return self.to_observation(self.state), reward, done, {}\n\n    def to_observation(self, state):\n        x, y = state\n        return y * self.grid_width + x\n\n    def render(self):\n        grid = np.full((self.grid_height, self.grid_width), ' ')\n        for y in range(self.grid_height):\n            for x in range(self.grid_width):\n                grid[y, x] = self.grid[y][x]\n        x, y = self.state\n        grid[y, x] = 'A'\n        print('\\n'.join(' '.join(row) for row in grid))\n        print()\n\ndef make_frozenlake(grid):\n    return FrozenLakeEnv(grid)\n\n# Example grid\nsafe_3x3 = [\n    ['S', 'X', '.'],\n    ['.', '.', '.'],\n    ['.', '.', 'G']\n]\n\n# env = make_frozenlake(safe_3x3) # 3x3 grid call","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\ngc.collect()\ntorch.cuda.empty_cache()\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1' # Used for debugging; CUDA related errors shown immediately.\n\n# Seed everything for reproducible results\nseed = 2024\nnp.random.seed(seed)\nnp.random.default_rng(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReplayMemory:\n    def __init__(self, capacity, device):\n        self.capacity = capacity\n        self.device = device\n        self.states = deque(maxlen=capacity)\n        self.actions = deque(maxlen=capacity)\n        self.next_states = deque(maxlen=capacity)\n        self.rewards = deque(maxlen=capacity)\n        self.dones = deque(maxlen=capacity)\n\n    def store(self, state, action, next_state, reward, done):\n        self.states.append(state)\n        self.actions.append(action)\n        self.next_states.append(next_state)\n        self.rewards.append(reward)\n        self.dones.append(done)\n\n    def sample(self, batch_size):\n        indices = np.random.choice(len(self), size=batch_size, replace=False)\n        device = self.device\n\n        states = torch.stack([torch.as_tensor(self.states[i], dtype=torch.float32, device=device) for i in indices]).to(device)\n        actions = torch.as_tensor([self.actions[i] for i in indices], dtype=torch.long, device=device)\n        next_states = torch.stack([torch.as_tensor(self.next_states[i], dtype=torch.float32, device=device) for i in indices]).to(device)\n        rewards = torch.as_tensor([self.rewards[i] for i in indices], dtype=torch.float32, device=device)\n        dones = torch.as_tensor([self.dones[i] for i in indices], dtype=torch.bool, device=device)\n\n        return states, actions, next_states, rewards, dones\n\n    def __len__(self):\n        return len(self.dones)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class C51Network(nn.Module):\n    def __init__(self, num_actions, input_dim, support_size, v_min, v_max):\n        super(C51Network, self).__init__()\n        self.support_size = support_size\n        self.num_actions = num_actions\n        self.v_min = v_min\n        self.v_max = v_max\n        self.delta_z = (v_max - v_min) / (support_size - 1)\n        self.support = torch.linspace(v_min, v_max, support_size).to(device)\n\n        self.FC = nn.Sequential(\n            nn.Linear(input_dim, 12),\n            nn.ReLU(inplace=True),\n            nn.Linear(12, 8),\n            nn.ReLU(inplace=True),\n            nn.Linear(8, num_actions * support_size)\n        )\n        \n#         self.FC = nn.Sequential(\n#             nn.Linear(input_dim, 120),\n#             nn.ReLU(),\n#             nn.Linear(120, 84),\n#             nn.ReLU(),\n#             nn.Linear(84, num_actions * support_size),\n#         )\n\n        # Initialize FC layer weights using He initialization\n        for layer in [self.FC]:\n            for module in layer:\n                if isinstance(module, nn.Linear):\n                    nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n\n    def forward(self, x):\n        x = self.FC(x)\n        x = x.view(-1, self.num_actions, self.support_size)\n        x = F.softmax(x, dim=2)\n        return x\n\n    def get_q_values(self, dist):\n        q_values = torch.sum(dist * self.support, dim=2)\n#         print(\"q_values\", q_values)\n        return q_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class C51Agent:\n    def __init__(self, env, seed, device, epsilon_max, epsilon_min, epsilon_decay,\n                 clip_grad_norm, learning_rate, discount, memory_capacity, support_size, v_min, v_max):\n        self.loss_history = []\n        self.running_loss = 0\n        self.learned_counts = 0\n\n        self.epsilon_max = epsilon_max\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.discount = discount\n\n        self.action_space = env.action_space\n        self.action_space.seed(seed)\n        self.observation_space = env.observation_space\n        self.replay_memory = ReplayMemory(memory_capacity, device)\n\n        self.support_size = support_size\n        self.v_min = v_min\n        self.v_max = v_max\n        self.delta_z = (v_max - v_min) / (support_size - 1)\n\n        self.main_network = C51Network(num_actions=self.action_space.n, input_dim=self.observation_space.n,\n                                       support_size=support_size, v_min=v_min, v_max=v_max).to(device)\n        self.target_network = C51Network(num_actions=self.action_space.n, input_dim=self.observation_space.n,\n                                         support_size=support_size, v_min=v_min, v_max=v_max).to(device).eval()\n        self.target_network.load_state_dict(self.main_network.state_dict())\n\n        self.clip_grad_norm = clip_grad_norm\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(self.main_network.parameters(), lr=learning_rate)\n\n    def select_action(self, state):\n        if np.random.random() < self.epsilon_max:\n#             print(\"action_space\", self.action_space)\n            return self.action_space.sample()\n        else:\n            with torch.no_grad():\n                Q_values = self.main_network.get_q_values(self.main_network(state))\n                action = torch.argmax(Q_values).item()\n#                 print(f\"State: {state}, Selected action: {action}\")\n                return action\n\n    def learn(self, batch_size, done):\n        if len(self.replay_memory) < batch_size:\n            return\n\n        states, actions, next_states, rewards, dones = self.replay_memory.sample(batch_size)\n        actions = actions.unsqueeze(1)\n        rewards = rewards.unsqueeze(1)\n        dones = dones.unsqueeze(1).float()\n\n        predicted_dist = self.main_network(states).gather(1, actions.unsqueeze(1).expand(-1, -1, self.support_size)).squeeze(1)\n\n        with torch.no_grad():\n            next_dist = self.target_network(next_states)\n            next_q_values = self.target_network.get_q_values(next_dist)\n            next_actions = next_q_values.max(1)[1]\n#             print(\"next_actions:\", next_actions)\n            next_dist = next_dist[range(batch_size), next_actions]\n\n            Tz = rewards + (1 - dones) * self.discount * self.main_network.support.unsqueeze(0)\n            Tz = Tz.clamp(min=self.v_min, max=self.v_max)\n            b = (Tz - self.v_min) / self.delta_z\n            l = b.floor().long()\n            u = b.ceil().long()\n\n            m = torch.zeros(batch_size, self.support_size).to(device)\n            offset = torch.linspace(0, (batch_size - 1) * self.support_size, batch_size).unsqueeze(1).expand(batch_size, self.support_size).to(device)\n            m.view(-1).index_add_(0, (l + offset).view(-1).long(), (next_dist * (u.float() - b)).view(-1))\n            m.view(-1).index_add_(0, (u + offset).view(-1).long(), (next_dist * (b - l.float())).view(-1))\n#             for i in range(batch_size):\n#                 for j in range(self.support_size):\n#                     m[i, l[i, j]] += next_dist[i, j] * (u[i, j] - b[i, j])\n#                     m[i, u[i, j]] += next_dist[i, j] * (b[i, j] - l[i, j])\n\n\n            \n\n        loss = -torch.sum(m * predicted_dist.log(), dim=1).mean()\n        self.optimizer.zero_grad()\n        loss.backward()\n#         print(f\"Loss: {loss.item()}, Epsilon: {self.epsilon_max}\")\n        torch.nn.utils.clip_grad_norm_(self.main_network.parameters(), self.clip_grad_norm)\n        self.optimizer.step()\n\n        self.running_loss += loss.item()\n        self.learned_counts += 1\n        if done:\n            episode_loss = self.running_loss / self.learned_counts\n            self.loss_history.append(episode_loss)\n            self.running_loss = 0\n            self.learned_counts = 0\n\n    def hard_update(self):\n        self.target_network.load_state_dict(self.main_network.state_dict())\n\n    def update_epsilon(self):\n        self.epsilon_max = max(self.epsilon_min, self.epsilon_max * self.epsilon_decay)\n\n    def save(self, path):\n        torch.save(self.main_network.state_dict(), path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model_TrainTest:\n    def __init__(self, seed, device, hyperparams, agent_type='c51'):\n        # Define RL Hyperparameters\n        self.train_mode             = hyperparams[\"train_mode\"]\n        self.RL_load_path           = hyperparams[\"RL_load_path\"]\n        self.save_path              = hyperparams[\"save_path\"]\n        self.save_interval          = hyperparams[\"save_interval\"]\n        \n        self.clip_grad_norm         = hyperparams[\"clip_grad_norm\"]\n        self.learning_rate          = hyperparams[\"learning_rate\"]\n        self.discount_factor        = hyperparams[\"discount_factor\"]\n        self.batch_size             = hyperparams[\"batch_size\"]\n        self.update_frequency       = hyperparams[\"update_frequency\"]\n        self.max_episodes           = hyperparams[\"max_episodes\"]\n        self.max_episode_length     = hyperparams[\"max_episode_length\"]\n        self.max_steps              = hyperparams[\"max_steps\"]\n        self.render                 = hyperparams[\"render\"]\n        \n        self.epsilon_max            = hyperparams[\"epsilon_max\"]\n        self.epsilon_min            = hyperparams[\"epsilon_min\"]\n        self.epsilon_decay          = hyperparams[\"epsilon_decay\"]\n        \n        self.memory_capacity        = hyperparams[\"memory_capacity\"]\n        \n        self.num_states             = hyperparams[\"num_states\"]\n        # self.map_size               = hyperparams[\"map_size\"]\n        self.map                    = hyperparams[\"map\"]\n        self.render_fps             = hyperparams[\"render_fps\"]\n        \n        self.n_bins                 = hyperparams[\"n_bins\"]\n        \n        self.env = make_frozenlake(\n            self.map)\n        self.env.metadata['render_fps'] = self.render_fps # For max frame rate make it 0\n        self.agent_type = agent_type\n        # Define the agent class\n        self.agent = None\n        self.reset_agent()\n        self.agent_type = agent_type\n        self.state_history = []\n        self.loss_bins = []\n\n    def state_preprocess(self, state:int, num_states:int):\n        \"\"\"\n        Convert an state to a tensor and basically it encodes the state into\n        an onehot vector. For example, the return can be something like tensor([0,0,1,0,0])\n        which could mean agent is at state 2 from total of 5 states.\n\n        \"\"\"\n        onehot_vector = torch.zeros(num_states, dtype=torch.float32, device=device)\n        onehot_vector[state] = 1\n        return onehot_vector\n    \n    \n    def reset_agent(self):\n#         if self.agent_type == 'dqn':\n#             self.agent = DQN_Agent(env            = self.env,\n#                                 seed              = seed,\n#                                 device            = device,\n#                                 epsilon_max       = self.epsilon_max,\n#                                 epsilon_min       = self.epsilon_min,\n#                                 epsilon_decay     = self.epsilon_decay,\n#                                 clip_grad_norm    = self.clip_grad_norm,\n#                                 learning_rate     = self.learning_rate,\n#                                 discount          = self.discount_factor,\n#                                 memory_capacity   = self.memory_capacity)\n#         if self.agent_type == 'ucb':\n#             self.agent = UCB_Agent(env            = self.env,\n#                                 seed              = seed,\n#                                 device            = device,\n#                                 epsilon_max       = self.epsilon_max,\n#                                 epsilon_min       = self.epsilon_min,\n#                                 epsilon_decay     = self.epsilon_decay,\n#                                 clip_grad_norm    = self.clip_grad_norm,\n#                                 learning_rate     = self.learning_rate,\n#                                 discount          = self.discount_factor,\n#                                 memory_capacity   = self.memory_capacity)\n#         if self.agent_type == 'ids':\n#             self.agent = IDS_Agent(   env               = self.env,\n#                                 seed              = seed,\n#                                 device            = device,\n#                                 epsilon_max       = self.epsilon_max,\n#                                 epsilon_min       = self.epsilon_min,\n#                                 epsilon_decay     = self.epsilon_decay,\n#                                 clip_grad_norm    = self.clip_grad_norm,\n#                                 learning_rate     = self.learning_rate,\n#                                 discount          = self.discount_factor,\n#                                 memory_capacity   = self.memory_capacity,\n#                                 num_ensembles     = 10)\n        if self.agent_type == 'c51':\n            support_size = 51 # n atoms\n            v_min = -10\n            v_max = 10\n            self.agent = C51Agent(env=self.env, \n                                  seed=seed, \n                                  device=device, \n                                  epsilon_max=self.epsilon_max,\n                                  epsilon_min=self.epsilon_min, \n                                  epsilon_decay=self.epsilon_decay,\n                                  clip_grad_norm=self.clip_grad_norm, \n                                  learning_rate=self.learning_rate,\n                                  discount=self.discount_factor,\n                                  memory_capacity=self.memory_capacity,\n                                  support_size=support_size,\n                                  v_min=v_min, \n                                  v_max=v_max)\n    \n    def run(self, training, agent=None):\n        \"\"\"                \n        Reinforcement learning training loop.\n        \"\"\"        \n        total_steps = 0\n        episode = 0\n        self.reward_history = []\n        self.episode_length_history = []\n        self.environment_steps_history = []\n        if not training:\n            self.agent = agent\n        else:\n            self.reset_agent()\n\n        # Training loop over episodes\n        while total_steps < self.max_steps and episode < self.max_episodes:\n            state = self.env.reset()\n            self.state_history.append(state)\n            state = self.state_preprocess(state, num_states=self.num_states)\n            done = False\n            truncation = False\n            step_size = 0\n            episode_reward = 0\n                                                \n            while not done and not truncation:\n                action = self.agent.select_action(state)\n                next_state, reward, done, truncation = self.env.step(action)\n                self.state_history.append(next_state)\n                next_state = self.state_preprocess(next_state, num_states=self.num_states)\n\n                if(training):\n                    self.agent.replay_memory.store(state, action, next_state, reward, done)\n\n                    # if len(self.agent.replay_memory) > self.batch_size and sum(self.reward_history) > 0: This was the original but since we have a negative reward so the sum doesn't work the same way anymore\n                    if len(self.agent.replay_memory) > self.batch_size:\n                        self.agent.learn(self.batch_size, (done or truncation))\n\n                        # Update target-network weights\n                        if total_steps % self.update_frequency == 0:\n                            self.agent.hard_update()\n\n                    # Penalize if the episode is truncated (BAD PRACTICE, NON MARKOVIAN)\n                    if not truncation and step_size >= self.max_episode_length:\n                        truncation = True\n                        reward = 0\n                \n                if self.render:\n                    self.env.render()\n                    print(f\"Step: {step_size}, State: {state}, Action: {action}, Reward: {reward}, Done: {done}\")\n\n                state = next_state\n                episode_reward += reward\n                step_size +=1\n                            \n            # Appends for tracking history\n            total_steps += step_size\n            self.reward_history.append(episode_reward) # episode reward    \n            self.episode_length_history.append(step_size) # episode length\n            self.environment_steps_history.append(total_steps) # total steps\n            episode += 1\n                                                                           \n            # Decay epsilon at the end of each episode\n            self.agent.update_epsilon()\n            \n            result = (f\"Episode: {episode}, \"\n                      f\"Total Steps: {total_steps}, \"\n                      f\"Raw Reward: {episode_reward:.2f}, \" \n                      f\"Episode Length: {step_size}, \")\n#             print(result)\n            \n        return self.agent\n                                                                    \n\n    # def test(self, max_episodes):\n    #     \"\"\"\n    #     Reinforcement learning policy evaluation.\n    #     \"\"\"\n    #\n    #     # # Load the weights of the test_network\n    #     # self.agent.main_network.load_state_dict(torch.load(self.RL_load_path))\n    #     # self.agent.main_network.eval()\n    #\n    #     # Testing loop over episodes\n    #     for episode in range(1, max_episodes+1):\n    #         state = self.env.reset()\n    #         done = False\n    #         truncation = False\n    #         step_size = 0\n    #         episode_reward = 0\n    #\n    #         while not done and not truncation:\n    #             state = self.state_preprocess(state, num_states=self.num_states)\n    #             action = self.agent.select_action(state)\n    #             next_state, reward, done, truncation, _ = self.env.step(action)\n    #\n    #             state = next_state\n    #             episode_reward += reward\n    #             step_size += 1\n    #\n    #         # Print log\n    #         result = (f\"Episode: {episode}, \"\n    #                   f\"Steps: {step_size:}, \"\n    #                   f\"Reward: {episode_reward:.2f}, \")\n    #         print(result)\n    \n    def plot_training(self):\n        plot_binned_line_with_std([self.environment_steps_history], [self.reward_history], self.n_bins, y_label=\"Reward\", title=f\"Reward over time ({self.agent_type})\", plot_individuals=False)\n        plot_binned_line_with_std([self.environment_steps_history], [self.episode_length_history], self.n_bins, y_label=\"Episode Length\", title=f\"Episode Length over time ({self.agent_type})\", plot_individuals=False)\n        print(len(self.environment_steps_history[3:]), len(self.agent.loss_history))\n        plot_binned_line_with_std([self.environment_steps_history[3:]], [self.agent.loss_history], self.n_bins, y_label=\"Loss\", title=f\"Loss over time ({self.agent_type})\", plot_individuals=False)\n\n    def get_plotting_data(self):\n        padded_loss_history = self.agent.loss_history\n        if len(padded_loss_history) == 0:\n            padded_loss_history = []\n        else:\n            while len(padded_loss_history) < len(self.environment_steps_history):\n                padded_loss_history = [padded_loss_history[0]] + padded_loss_history\n        return self.environment_steps_history, self.reward_history, self.episode_length_history, padded_loss_history, self.state_history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_aleatoric_uncertainty_3x3 = [\n    ['S', '.', '.'],\n    ['.', '.', '.'],\n    ['.', '.', 'G']\n]\n\nsafe_3x3 = [\n    ['S', 'X', '.'],\n    ['.', '.', '.'],\n    ['.', '.', 'G']\n]\n\nlong_safe_4x3 = [\n    ['S', 'X', 'G'],\n    ['.', 'X', '.'],\n    ['.', 'X', '.'],\n    ['.', '.', '.']\n]\n\nshort_unsafe_long_safe_4x3 = [\n    ['S', 'X', 'G'],\n    ['.', '.', '.'],\n    ['.', 'X', '.'],\n    ['.', '.', '.']\n]\n\nunsafe_path_safe_area_3x4 = [\n    ['S', '.', 'X', '.'],\n    ['.', '.', '.', '.'],\n    ['.', '.', 'X', 'G']\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_results(env_steps_histories, eps_length_histories, map_state_histories, agent_reward_histories, agent_loss_histories, state_map, agents, n_bins):\n    if train_mode:\n        plot_binned_line_with_std(env_steps_histories, agent_reward_histories, n_bins, y_label=\"Reward\", title=\"Reward over time\", plot_individuals=True)\n        plot_binned_line_with_std(env_steps_histories, eps_length_histories, n_bins, y_label=\"Episode Length\", title=\"Episode Length over time\", plot_individuals=True)\n        plot_binned_line_with_std(env_steps_histories, agent_loss_histories, n_bins, y_label=\"Loss\", title=\"Loss over time\", plot_individuals=True)\n    else:\n        for i in range(len(map_state_histories)):\n            state_table = np.zeros_like(state_map)\n            print(agents[i].get_best_actions(state_table, device))\n            plot_grid_statespace(map_state_histories[i], agents[i].get_best_actions(state_table, device), state_map)\n        plot_barchart_rewards(agent_reward_histories, y_label=\"Reward Counts\", title=\"Bar Chart of Unique Values and Their Corresponding Reward Counts\")\n        plot_barchart_episode_length(eps_length_histories, n_bins, y_label='Amount of Finished Runs', title=\"Bar Chart of Unique Values and Their Corresponding Reward Counts\")\n        \ndef run_model(agents, agent_type):\n    DRL = Model_TrainTest(seed, device, RL_hyperparams, agent_type=agent_type) # Define the instance\n    environment_steps_histories, reward_histories, episode_length_histories, loss_histories, state_histories = [], [], [], [], []\n    if train_mode:\n        for i in range(10):\n            print(i)\n            trained_agent = DRL.run(train_mode, None)\n            agents.append(trained_agent)\n    \n            # Save all the data for plotting (plotted in next cell)\n            environment_steps_history, reward_history, episode_length_history, loss_history, state_history = DRL.get_plotting_data()\n            environment_steps_histories.append(environment_steps_history)\n            reward_histories.append(reward_history)\n            episode_length_histories.append(episode_length_history)\n            loss_histories.append(loss_history)\n            state_histories.append(state_history)\n    else:\n        for agent in agents:\n            DRL.run(train_mode, agent)\n            \n            # Save all the data for plotting (plotted in next cell)\n            environment_steps_history, reward_history, episode_length_history, loss_history, state_history = DRL.get_plotting_data()\n            environment_steps_histories.append(environment_steps_history)\n            reward_histories.append(reward_history)\n            episode_length_histories.append(episode_length_history)\n            loss_histories.append(loss_history)\n            state_histories.append(state_history)\n    return environment_steps_histories, reward_histories, episode_length_histories, loss_histories, state_histories, agents","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameters:\ntrain_mode = True\nagent_type = 'c51'               # 'dqn', 'ucb', 'ids' or 'c51'\n\nrender = not train_mode\n# map_size = 4 # 4x4 or 8x8 (outdated)\nmap_mame = \"safe_3x3\"\nstate_map = short_unsafe_long_safe_4x3\nRL_hyperparams = {\n    \"train_mode\"            : train_mode,\n    \"RL_load_path\"          : f'./level_stats/{map_mame}/final_weights' + '_' + '3000' + '.pth',\n    \"save_path\"             : f'./level_stats/{map_mame}/final_weights',\n    \"save_interval\"         : 500,\n    \n    \"clip_grad_norm\"        : 3,\n    \"learning_rate\"         : 8e-6,\n    \"discount_factor\"       : 0.94,\n    \"batch_size\"            : 64,\n    \"update_frequency\"      : 10,\n    \"max_episodes\"          : 100000           if train_mode else 200,\n    \"max_steps\"             : 300000,\n    \"max_episode_length\"    : 600,\n    \"render\"                : render,\n    \n    \"epsilon_max\"           : 0.999         if train_mode else -1,\n    \"epsilon_min\"           : 0.01,\n    \"epsilon_decay\"         : 0.999,\n    \n    \"memory_capacity\"       : 10_000        if train_mode else 0,\n        \n    # \"map_size\"              : map_size,\n    \"num_states\"            : len(state_map) * len(state_map[0]),    # 3 rows in your example\n    \"map\"                   : state_map,\n    \"render_fps\"            : 6,\n    \"n_bins\"                : 100\n    }\n\nagent_histories = []\nenvironment_steps_histories, reward_histories, episode_length_histories, loss_histories, state_histories, agent_histories = run_model(agent_histories, agent_type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nn_bins = 20\nplot_results(environment_steps_histories, episode_length_histories, state_histories, reward_histories, loss_histories, state_map, agent_histories, n_bins)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}