{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b8576572f8118258"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-27T08:58:05.527054Z",
     "start_time": "2024-05-27T08:57:59.906088Z"
    }
   },
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T08:58:05.559995Z",
     "start_time": "2024-05-27T08:58:05.527784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class FrozenLakeEnv(gym.Env):\n",
    "    def __init__(self, grid):\n",
    "        super(FrozenLakeEnv, self).__init__()\n",
    "        self.grid = grid\n",
    "        self.grid_height = len(grid)\n",
    "        self.grid_width = len(grid[0])\n",
    "        [self.start_y], [self.start_x] = np.where(np.array(grid) == 'S')\n",
    "        self.slip_probability = 1/3 # Probability to slip to one side (so the chance of slipping in any direction is 2 times this value)\n",
    "        assert self.slip_probability <= 1/2\n",
    "        self.action_space = spaces.Discrete(4)  # Left, Down, Right, Up\n",
    "        self.observation_space = spaces.Discrete(self.grid_height * self.grid_width)\n",
    "\n",
    "        self.state_action_count = {}\n",
    "        for x in range(self.grid_width):\n",
    "            for y in range(self.grid_height):\n",
    "                self.state_action_count[(x, y)] = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "\n",
    "    def reset(self):\n",
    "        # Top left corner is 0, 0\n",
    "        self.state = (self.start_x, self.start_y)\n",
    "        return self.to_observation(self.state)\n",
    "    \n",
    "    def get_aleatoric_uncertainty(self):\n",
    "        \n",
    "        \n",
    "        # Define possible actions for each chosen direction\n",
    "        # Make sure the first action in the array is the action itself (no slip)\n",
    "        possible_actions = {\n",
    "                0: [0, 3, 1],  # Left\n",
    "                1: [1, 0, 2],  # Down\n",
    "                2: [2, 1, 3],  # Right\n",
    "                3: [3, 2, 0]   # Up\n",
    "            }  \n",
    "        variance_map = []\n",
    "\n",
    "        # Choose a random action from the possible actions according to self.slip_probability\n",
    "        p = self.slip_probability\n",
    "\n",
    "        for state in range(self.grid_width*self.grid_height):\n",
    "            # Move in the chosen direction if its within bounds\n",
    "            variances = {0:[],1:[],2:[],3:[]}\n",
    "            # print(state)\n",
    "            for wanted_action in possible_actions:\n",
    "                for action in possible_actions[wanted_action]:\n",
    "                    x, y = state % self.grid_width, state // self.grid_height\n",
    "                    # print(x,y)\n",
    "                    if action == 0 and x > 0:\n",
    "                        x -= 1\n",
    "                    elif action == 1 and y < self.grid_height - 1:\n",
    "                        y += 1\n",
    "                    elif action == 2 and x < self.grid_width - 1:\n",
    "                        x += 1\n",
    "                    elif action == 3 and y > 0:\n",
    "                        y -= 1\n",
    "                    # Check state of the cell\n",
    "                    if self.grid[y][x] == 'X':\n",
    "                        reward = -1\n",
    "                    elif self.grid[y][x] == 'G':\n",
    "                        reward = 1\n",
    "                    else:\n",
    "                        reward = 0\n",
    "                    variances[wanted_action].append(reward)\n",
    "            # print(variances)\n",
    "            \n",
    "            for k in variances:\n",
    "                variances[k] = torch.tensor(np.var(variances[k]))\n",
    "            variances = torch.stack(tuple(variances[k] for k in variances))\n",
    "            variance_map.append(variances)\n",
    "\n",
    "        return variance_map\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state_action_count[self.state][action] += 1\n",
    "        x, y = self.state\n",
    "\n",
    "        # Define possible actions for each chosen direction\n",
    "        # Make sure the first action in the array is the action itself (no slip)\n",
    "        possible_actions = {\n",
    "                0: [0, 3, 1],  # Left\n",
    "                1: [1, 0, 2],  # Down\n",
    "                2: [2, 1, 3],  # Right\n",
    "                3: [3, 2, 0]   # Up\n",
    "            }            \n",
    "\n",
    "        # Choose a random action from the possible actions according to self.slip_probability\n",
    "        p = self.slip_probability\n",
    "        action = np.random.choice(possible_actions[action], p=[1-2*p, p, p])\n",
    "        # print(\"Actual action\", [\"left\", \"down\", \"right\", \"up\"][action])\n",
    "\n",
    "        # Move in the chosen direction if its within bounds\n",
    "        if action == 0 and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 1 and y < self.grid_height - 1:\n",
    "            y += 1\n",
    "        elif action == 2 and x < self.grid_width - 1:\n",
    "            x += 1\n",
    "        elif action == 3 and y > 0:\n",
    "            y -= 1\n",
    "\n",
    "        self.state = (x, y)\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        # Check state of the cell\n",
    "        if self.grid[y][x] == 'X':\n",
    "            reward = -5\n",
    "            done = True\n",
    "        elif self.grid[y][x] == 'G':\n",
    "            reward = 10\n",
    "            done = True\n",
    "\n",
    "        return self.to_observation(self.state), reward, done, {}\n",
    "\n",
    "    def to_observation(self, state):\n",
    "        x, y = state\n",
    "        return y * self.grid_width + x\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((self.grid_height, self.grid_width), ' ')\n",
    "        for y in range(self.grid_height):\n",
    "            for x in range(self.grid_width):\n",
    "                grid[y, x] = self.grid[y][x]\n",
    "        x, y = self.state\n",
    "        grid[y, x] = 'A'\n",
    "        print('\\n'.join(' '.join(row) for row in grid))\n",
    "        print()\n",
    "\n",
    "def make_frozenlake(grid):\n",
    "    return FrozenLakeEnv(grid)\n"
   ],
   "id": "864ba6cd0ac3689c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T08:58:05.642862Z",
     "start_time": "2024-05-27T08:58:05.560159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # Used for debugging; CUDA related errors shown immediately.\n",
    "\n",
    "# Seed everything for reproducible results\n",
    "seed = 2024\n",
    "np.random.seed(seed)\n",
    "np.random.default_rng(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    "
   ],
   "id": "f4e661617c431af7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T09:02:51.684063Z",
     "start_time": "2024-05-27T09:02:51.625942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Experience Replay Memory defined by deques to store transitions/agent experiences\n",
    "        \"\"\"\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        \n",
    "        self.states       = deque(maxlen=capacity)\n",
    "        self.actions      = deque(maxlen=capacity)\n",
    "        self.next_states  = deque(maxlen=capacity)\n",
    "        self.rewards      = deque(maxlen=capacity)\n",
    "        self.dones        = deque(maxlen=capacity)\n",
    "        \n",
    "        \n",
    "    def store(self, state, action, next_state, reward, done):\n",
    "        \"\"\"\n",
    "        Append (store) the transitions to their respective deques\n",
    "        \"\"\"\n",
    "        \n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.next_states.append(next_state)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample transitions from memory, then convert sampled transitions\n",
    "        to tensors and move to device (CPU or GPU).\n",
    "        \"\"\"\n",
    "        \n",
    "        indices = np.random.choice(len(self), size=batch_size, replace=False)\n",
    "\n",
    "        states = torch.stack([torch.as_tensor(self.states[i], dtype=torch.float32, device=device) for i in indices]).to(device)\n",
    "        actions = torch.as_tensor([self.actions[i] for i in indices], dtype=torch.long, device=device)\n",
    "        next_states = torch.stack([torch.as_tensor(self.next_states[i], dtype=torch.float32, device=device) for i in indices]).to(device)\n",
    "        rewards = torch.as_tensor([self.rewards[i] for i in indices], dtype=torch.float32, device=device)\n",
    "        dones = torch.as_tensor([self.dones[i] for i in indices], dtype=torch.bool, device=device)\n",
    "\n",
    "        return states, actions, next_states, rewards, dones\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        To check how many samples are stored in the memory. self.dones deque \n",
    "        represents the length of the entire memory.\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.dones)\n",
    "    \n",
    "    \n",
    "class DQN_Network(nn.Module):\n",
    "    \"\"\"\n",
    "    The Deep Q-Network (DQN) model for reinforcement learning.\n",
    "    This network consists of Fully Connected (FC) layers with ReLU activation functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions, input_dim):\n",
    "        \"\"\"\n",
    "        Initialize the DQN network.\n",
    "        \n",
    "        Parameters:\n",
    "            num_actions (int): The number of possible actions in the environment.\n",
    "            input_dim (int): The dimensionality of the input state space.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(DQN_Network, self).__init__()\n",
    "                                                          \n",
    "        self.FC = nn.Sequential(\n",
    "            nn.Linear(input_dim, 12),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(12, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, num_actions)\n",
    "            )\n",
    "        \n",
    "        # Initialize FC layer weights using He initialization\n",
    "        for layer in [self.FC]:\n",
    "            for module in layer:\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network to find the Q-values of the actions.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor representing the state.\n",
    "        \n",
    "        Returns:\n",
    "            Q (torch.Tensor): Tensor containing Q-values for each action.\n",
    "        \"\"\"\n",
    "        \n",
    "        Q = self.FC(x)    \n",
    "        return Q\n",
    "    \n",
    "        \n",
    "class IDS:\n",
    "    \"\"\"\n",
    "    DQN Agent Class. This class defines some key elements of the DQN algorithm,\n",
    "    such as the learning method, hard update, and action selection based on the\n",
    "    Q-value of actions or the epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, epsilon_max, epsilon_min, epsilon_decay, \n",
    "                  clip_grad_norm, learning_rate, discount, memory_capacity, num_ensembles):\n",
    "        \n",
    "        # To save the history of network loss\n",
    "        self.loss_history = [[] for _ in range(num_ensembles)]\n",
    "        self.running_loss = [0 for _ in range(num_ensembles)]\n",
    "        self.learned_counts = [0 for _ in range(num_ensembles)]\n",
    "                     \n",
    "        # RL hyperparameters\n",
    "        self.epsilon_max   = epsilon_max\n",
    "        self.epsilon_min   = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.discount      = discount\n",
    "        self.env = env\n",
    "        self.aleatoric_map = env.get_aleatoric_uncertainty()\n",
    "\n",
    "        self.action_space  = env.action_space\n",
    "        self.action_space.seed(seed) # Set the seed to get reproducible results when sampling the action space \n",
    "        self.observation_space = env.observation_space\n",
    "        self.replay_memory = ReplayMemory(memory_capacity)\n",
    "        \n",
    "        # Initiate the network models\n",
    "        self.models = [DQN_Network(num_actions=self.action_space.n, input_dim=self.observation_space.n).to(device) for _ in range(num_ensembles)]\n",
    "        self.targets = [DQN_Network(num_actions=self.action_space.n, input_dim=self.observation_space.n).to(device).eval() for _ in range(num_ensembles)]\n",
    "        for target, main in zip(self.targets, self.models):\n",
    "            target.load_state_dict(main.state_dict())\n",
    "            \n",
    "        self.clip_grad_norm = clip_grad_norm # For clipping exploding gradients caused by high reward value\n",
    "        self.critertion = [nn.MSELoss() for _ in range(num_ensembles)]\n",
    "        self.optimizers = [optim.Adam(model.parameters(), lr=learning_rate) for model in self.models]\n",
    "                \n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon-greedy strategy OR based on the Q-values.\n",
    "        \n",
    "        Parameters:\n",
    "            state (torch.Tensor): Input tensor representing the state.\n",
    "        \n",
    "        Returns:\n",
    "            action (int): The selected action.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Exploitation: the action is selected based on the Q-values.    \n",
    "        with torch.no_grad():\n",
    "            q_values = torch.stack(tuple(model(state) for model in self.models))\n",
    "            \n",
    "            means = torch.mean(q_values,dim=0)\n",
    "            std = torch.std(q_values,dim=0)\n",
    "            var = torch.var(q_values,dim=0)\n",
    "                            \n",
    "            \n",
    "            ub = means + 0.1*std\n",
    "            lb = means - 0.1*std\n",
    "            \n",
    "            a_star = torch.argmax(ub).item()\n",
    "            ub_star = torch.max(ub)\n",
    "            \n",
    "            regret = ub_star - lb\n",
    "            \n",
    "\n",
    "            aleatoric = self.aleatoric_map[torch.argmax(state).item()]\n",
    "            denom = torch.mean(aleatoric) + 1e-5\n",
    "            aleatoric = aleatoric/denom\n",
    "            \n",
    "            # this line causes a problem \n",
    "            info_gain = torch.log(1 + (var/aleatoric))\n",
    "\n",
    "            info_gain = var\n",
    "            \n",
    "            ids = (regret**2)/info_gain\n",
    "            \n",
    "            # ids[a_star] = float(\"inf\")\n",
    "            \n",
    "            action = torch.argmin(ids).item()\n",
    "\n",
    "            return action\n",
    "   \n",
    "\n",
    "    def learn(self, batch_size, done):\n",
    "        \"\"\"\n",
    "        Train the main network using a batch of experiences sampled from the replay memory.\n",
    "        \n",
    "        Parameters:\n",
    "            batch_size (int): The number of experiences to sample from the replay memory.\n",
    "            done (bool): Indicates whether the episode is done or not. If done,\n",
    "            calculate the loss of the episode and append it in a list for plot.\n",
    "        \"\"\" \n",
    "        \n",
    "        # Sample a batch of experiences from the replay memory\n",
    "        states, actions, next_states, rewards, dones = self.replay_memory.sample(batch_size)\n",
    "                    \n",
    "        \n",
    "        # # The following prints are for debugging. Use them to indicate the correct shape of the tensors.\n",
    "        # print('Before--------Before')\n",
    "        # print(\"states:\", states.shape)\n",
    "        # print(\"actions:\", actions.shape)\n",
    "        # print(\"next_states:\", next_states.shape)\n",
    "        # print(\"rewards:\", rewards.shape)\n",
    "        # print(\"dones:\", dones.shape)\n",
    "               \n",
    "         \n",
    "        # # Preprocess the data for training\n",
    "        # states        = states.unsqueeze(1)\n",
    "        # next_states   = next_states.unsqueeze(1)\n",
    "        actions       = actions.unsqueeze(1)\n",
    "        rewards       = rewards.unsqueeze(1)\n",
    "        dones         = dones.unsqueeze(1)       \n",
    "        \n",
    "        \n",
    "        # # The following prints are for debugging. Use them to indicate the correct shape of the tensors.\n",
    "        # print()\n",
    "        # print('After--------After')\n",
    "        # print(\"states:\", states.shape)\n",
    "        # print(\"actions:\", actions.shape)\n",
    "        # print(\"next_states:\", next_states.shape)\n",
    "        # print(\"rewards:\", rewards.shape)\n",
    "        # print(\"dones:\", dones.shape)\n",
    "        \n",
    "        for model, target, optimizer, index, critertion in zip(self.models, self.targets, self.optimizers, range(len(self.models)), self.critertion):\n",
    "            \n",
    "            predicted_q = model(states) # forward pass through the main network to find the Q-values of the states\n",
    "            predicted_q = predicted_q.gather(dim=1, index=actions) # selecting the Q-values of the actions that were actually taken\n",
    "    \n",
    "            # Compute the maximum Q-value for the next states using the target network\n",
    "            with torch.no_grad():            \n",
    "                next_target_q_value = target(next_states).max(dim=1, keepdim=True)[0] # not argmax (cause we want the maxmimum q-value, not the action that maximize it)\n",
    "                \n",
    "            \n",
    "            next_target_q_value[dones] = 0 # Set the Q-value for terminal states to zero\n",
    "            y_js = rewards + (self.discount * next_target_q_value) # Compute the target Q-values\n",
    "            loss = critertion(predicted_q, y_js) # Compute the loss\n",
    "            \n",
    "            # Update the running loss and learned counts for logging and plotting\n",
    "            self.running_loss[index] += loss.item()\n",
    "            self.learned_counts[index] += 1\n",
    "    \n",
    "            if done:\n",
    "                episode_loss = self.running_loss[index] / self.learned_counts[index] # The average loss for the episode\n",
    "                # print(episode_loss)\n",
    "                self.loss_history[index].append(episode_loss) # Append the episode loss to the loss history for plotting\n",
    "                # Reset the running loss and learned counts\n",
    "                self.running_loss[index] = 0\n",
    "                self.learned_counts[index] = 0\n",
    "                \n",
    "            optimizer.zero_grad() # Zero the gradients\n",
    "            loss.backward() # Perform backward pass and update the gradients\n",
    "            \n",
    "            # # Uncomment the following two lines to find the best value for clipping gradient (Comment torch.nn.utils.clip_grad_norm_ while uncommenting the following two lines)\n",
    "            # grad_norm_before_clip = torch.nn.utils.clip_grad_norm_(self.main_network.parameters(), float('inf'))\n",
    "            # print(\"Gradient norm before clipping:\", grad_norm_before_clip)\n",
    "            \n",
    "            # Clip the gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_grad_norm)\n",
    "            \n",
    "            optimizer.step() # Update the parameters of the main network using the optimizer\n",
    "        \n",
    "        # print(self.loss_history)\n",
    " \n",
    "\n",
    "    def hard_update(self):\n",
    "        \"\"\"\n",
    "        Navie update: Update the target network parameters by directly copying \n",
    "        the parameters from the main network.\n",
    "        \"\"\"\n",
    "        \n",
    "        for model, target in zip(self.models, self.targets):\n",
    "            target.load_state_dict(model.state_dict())\n",
    "\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"\n",
    "        Update the value of epsilon for epsilon-greedy exploration.\n",
    "        \n",
    "        This method decreases epsilon over time according to a decay factor, ensuring\n",
    "        that the agent becomes less exploratory and more exploitative as training progresses.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.epsilon_max = max(self.epsilon_min, self.epsilon_max * self.epsilon_decay)\n",
    "        \n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the parameters of the main network to a file with .pth extention.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        for model in self.models:\n",
    "            torch.save(model.state_dict(), path)\n",
    "                  \n",
    "\n",
    "class Model_TrainTest:\n",
    "    def __init__(self, hyperparams):\n",
    "        \n",
    "        # Define RL Hyperparameters\n",
    "        self.train_mode             = hyperparams[\"train_mode\"]\n",
    "        self.RL_load_path           = hyperparams[\"RL_load_path\"]\n",
    "        self.save_path              = hyperparams[\"save_path\"]\n",
    "        self.save_interval          = hyperparams[\"save_interval\"]\n",
    "        \n",
    "        self.clip_grad_norm         = hyperparams[\"clip_grad_norm\"]\n",
    "        self.learning_rate          = hyperparams[\"learning_rate\"]\n",
    "        self.discount_factor        = hyperparams[\"discount_factor\"]\n",
    "        self.batch_size             = hyperparams[\"batch_size\"]\n",
    "        self.update_frequency       = hyperparams[\"update_frequency\"]\n",
    "        self.max_episodes           = hyperparams[\"max_episodes\"]\n",
    "        self.max_steps              = hyperparams[\"max_steps\"]\n",
    "        self.render                 = hyperparams[\"render\"]\n",
    "        \n",
    "        self.epsilon_max            = hyperparams[\"epsilon_max\"]\n",
    "        self.epsilon_min            = hyperparams[\"epsilon_min\"]\n",
    "        self.epsilon_decay          = hyperparams[\"epsilon_decay\"]\n",
    "        \n",
    "        self.memory_capacity        = hyperparams[\"memory_capacity\"]\n",
    "        \n",
    "        self.num_states             = hyperparams[\"num_states\"]\n",
    "        # self.map_size               = hyperparams[\"map_size\"]\n",
    "        self.map                    = hyperparams[\"map\"]\n",
    "        self.render_fps             = hyperparams[\"render_fps\"]\n",
    "                        \n",
    "        # Define Env\n",
    "        # self.env = gym.make('FrozenLake-v1', map_name=f\"{self.map_size}x{self.map_size}\", \n",
    "        #                     is_slippery=False, max_episode_steps=self.max_steps, \n",
    "        #                     render_mode=\"human\" if self.render else None)\n",
    "        \n",
    "        self.env = make_frozenlake(self.map)\n",
    "        self.env.metadata['render_fps'] = self.render_fps # For max frame rate make it 0\n",
    "        \n",
    "        # Define the agent class\n",
    "        self.agent = IDS(env                = self.env, \n",
    "                                epsilon_max       = self.epsilon_max, \n",
    "                                epsilon_min       = self.epsilon_min, \n",
    "                                epsilon_decay     = self.epsilon_decay,\n",
    "                                clip_grad_norm    = self.clip_grad_norm,\n",
    "                                learning_rate     = self.learning_rate,\n",
    "                                discount          = self.discount_factor,\n",
    "                                memory_capacity   = self.memory_capacity,\n",
    "                                num_ensembles    = 10\n",
    "    )\n",
    "                \n",
    "        \n",
    "    def state_preprocess(self, state:int, num_states:int):\n",
    "        \"\"\"\n",
    "        Convert an state to a tensor and basically it encodes the state into \n",
    "        an onehot vector. For example, the return can be something like tensor([0,0,1,0,0]) \n",
    "        which could mean agent is at state 2 from total of 5 states.\n",
    "\n",
    "        \"\"\"\n",
    "        onehot_vector = torch.zeros(num_states, dtype=torch.float32, device=device)\n",
    "        onehot_vector[state] = 1\n",
    "        return onehot_vector\n",
    "    \n",
    "    \n",
    "    def train(self): \n",
    "        \"\"\"                \n",
    "        Reinforcement learning training loop.\n",
    "        \"\"\"\n",
    "        \n",
    "        total_steps = 0\n",
    "        self.reward_history = []\n",
    "        \n",
    "        # Training loop over episodes\n",
    "        for episode in range(1, self.max_episodes+1):\n",
    "            # state, _ = self.env.reset(seed=seed)\n",
    "            state = self.env.reset()\n",
    "            state = self.state_preprocess(state, num_states=self.num_states)\n",
    "            done = False\n",
    "            truncation = False\n",
    "            step_size = 0\n",
    "            episode_reward = 0\n",
    "                                                \n",
    "            while not done and not truncation:\n",
    "                action = self.agent.select_action(state)\n",
    "                # next_state, reward, done, truncation, _ = self.env.step(action)\n",
    "                next_state, reward, done, truncation = self.env.step(action)\n",
    "                next_state = self.state_preprocess(next_state, num_states=self.num_states)\n",
    "                \n",
    "                self.agent.replay_memory.store(state, action, next_state, reward, done) \n",
    "                \n",
    "                if len(self.agent.replay_memory) > self.batch_size:\n",
    "                    self.agent.learn(self.batch_size, (done or truncation))\n",
    "                    \n",
    "                \n",
    "                    # Update target-network weights\n",
    "                    if total_steps % self.update_frequency == 0:\n",
    "                        self.agent.hard_update()\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step_size +=1\n",
    "                            \n",
    "            # Appends for tracking history\n",
    "            self.reward_history.append(episode_reward) # episode reward                        \n",
    "            total_steps += step_size\n",
    "                                                                           \n",
    "            # Decay epsilon at the end of each episode\n",
    "            self.agent.update_epsilon()\n",
    "                            \n",
    "            #-- based on interval\n",
    "            if episode % self.save_interval == 0:\n",
    "                self.agent.save(self.save_path + '_' + f'{episode}' + '.pth')\n",
    "                # if episode != self.max_episodes:\n",
    "                #     # self.plot_training(episode)\n",
    "                print('\\n~~~~~~Interval Save: Model saved.\\n')\n",
    "    \n",
    "            result = (f\"Episode: {episode}, \"\n",
    "                      f\"Total Steps: {total_steps}, \"\n",
    "                      f\"Ep Step: {step_size}, \"\n",
    "                      f\"Raw Reward: {episode_reward:.2f}, \"\n",
    "                      f\"Epsilon: {self.agent.epsilon_max:.2f}\")\n",
    "            print(result)\n",
    "            # print(self.agent.loss_history)\n",
    "        self.plot_training(episode)\n",
    "                                                                    \n",
    "\n",
    "    def test(self, max_episodes):  \n",
    "        \"\"\"                \n",
    "        Reinforcement learning policy evaluation.\n",
    "        \"\"\"\n",
    "           \n",
    "        # Load the weights of the test_network\n",
    "        for model in self.agent.models:\n",
    "            model.load_state_dict(torch.load(self.RL_load_path))\n",
    "            model.eval()\n",
    "        \n",
    "        # Testing loop over episodes\n",
    "        for episode in range(1, max_episodes+1):         \n",
    "            # state, _ = self.env.reset(seed=seed)\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            truncation = False\n",
    "            step_size = 0\n",
    "            episode_reward = 0\n",
    "                                                           \n",
    "            while not done and not truncation:\n",
    "                state = self.state_preprocess(state, num_states=self.num_states)\n",
    "                action = self.agent.select_action(state)\n",
    "                next_state, reward, done, truncation, _ = self.env.step(action)\n",
    "                                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step_size += 1\n",
    "                                                                                                                       \n",
    "            # Print log            \n",
    "            result = (f\"Episode: {episode}, \"\n",
    "                      f\"Steps: {step_size:}, \"\n",
    "                      f\"Reward: {episode_reward:.2f}, \")\n",
    "            print(result)\n",
    "            \n",
    "        pygame.quit() # close the rendering window\n",
    "        \n",
    "    \n",
    "    def plot_training(self, episode):\n",
    "        # print(self.agent.loss_history)\n",
    "        # Calculate the Simple Moving Average (SMA) with a window size of 50\n",
    "        sma = np.convolve(self.reward_history, np.ones(50)/50, mode='valid')\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title(\"Rewards\")\n",
    "        plt.plot(self.reward_history, label='Raw Reward', color='#F6CE3B', alpha=1)\n",
    "        plt.plot(sma, label='SMA 50', color='#385DAA')\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Rewards\")\n",
    "        plt.legend()\n",
    "        \n",
    "        # Only save as file if last episode\n",
    "        if episode == self.max_episodes:\n",
    "            plt.savefig('./reward_plot.png', format='png', dpi=600, bbox_inches='tight')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close() \n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(np.mean(self.agent.loss_history, axis=0), label='Loss', color='#CB291A', alpha=1)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        \n",
    "        # Only save as file if last episode\n",
    "        if episode == self.max_episodes:\n",
    "            plt.savefig('./Loss_plot.png', format='png', dpi=600, bbox_inches='tight')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()        \n",
    "        "
   ],
   "id": "3e42e0341b309258",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T09:02:51.901828Z",
     "start_time": "2024-05-27T09:02:51.885639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "no_aleatoric_uncertainty_3x3 = [\n",
    "    ['.', 'S', '.'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', '.', 'G']\n",
    "]\n",
    "\n",
    "safe_3x3 = [\n",
    "    ['S', 'X', '.'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', '.', 'G']\n",
    "]\n",
    "\n",
    "long_safe_4x3 = [\n",
    "    ['S', 'X', 'G'],\n",
    "    ['.', 'X', '.'],\n",
    "    ['.', 'X', '.'],\n",
    "    ['.', '.', '.']\n",
    "]\n",
    "\n",
    "short_unsafe_long_safe_4x3 = [\n",
    "    ['S', 'X', 'G'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', 'X', '.'],\n",
    "    ['.', '.', '.']\n",
    "]\n",
    "\n",
    "unsafe_path_safe_area_3x4 = [\n",
    "    ['S', '.', 'X', '.'],\n",
    "    ['.', '.', '.', '.'],\n",
    "    ['.', '.', 'X', 'G']\n",
    "]"
   ],
   "id": "917d5edc35ee5a25",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-27T09:02:52.400530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters:\n",
    "train_mode = True\n",
    "render = not train_mode\n",
    "# map_size = 4 # 4x4 or 8x8 (outdated)\n",
    "map_mame = \"no_aleatoric_uncertainty_3x3\"\n",
    "RL_hyperparams = {\n",
    "    \"train_mode\"            : train_mode,\n",
    "    \"RL_load_path\"          : f'./level_stats/{map_mame}/final_weights' + '_' + '3000' + '.pth',\n",
    "    \"save_path\"             : f'./level_stats/{map_mame}/final_weights',\n",
    "    \"save_interval\"         : 500,\n",
    "    \n",
    "    \"clip_grad_norm\"        : 3,\n",
    "    \"learning_rate\"         : 6e-4,\n",
    "    \"discount_factor\"       : 0.93,\n",
    "    \"batch_size\"            : 32,\n",
    "    \"update_frequency\"      : 10,\n",
    "    \"max_episodes\"          : 1000          if train_mode else 5,\n",
    "    \"max_steps\"             : 2000,\n",
    "    \"render\"                : render,\n",
    "    \n",
    "    \"epsilon_max\"           : 0.999         if train_mode else -1,\n",
    "    \"epsilon_min\"           : 0.01,\n",
    "    \"epsilon_decay\"         : 0.999,\n",
    "    \n",
    "    \"memory_capacity\"       : 4_000        if train_mode else 0,\n",
    "        \n",
    "    # \"map_size\"              : map_size,\n",
    "    \"num_states\"            : 3*3,\n",
    "    \"map\"                   : safe_3x3,\n",
    "    \"render_fps\"            : 6,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run\n",
    "DRL = Model_TrainTest(RL_hyperparams) # Define the instance\n",
    "# Train\n",
    "if train_mode:\n",
    "    DRL.train()\n",
    "else:\n",
    "    # Test\n",
    "    DRL.test(max_episodes = RL_hyperparams['max_episodes'])"
   ],
   "id": "b15db93d78c5b8c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Steps: 71, Ep Step: 71, Raw Reward: 10.00, Epsilon: 1.00\n",
      "Episode: 2, Total Steps: 114, Ep Step: 43, Raw Reward: 10.00, Epsilon: 1.00\n",
      "Episode: 3, Total Steps: 162, Ep Step: 48, Raw Reward: -5.00, Epsilon: 1.00\n",
      "Episode: 4, Total Steps: 170, Ep Step: 8, Raw Reward: 10.00, Epsilon: 1.00\n",
      "Episode: 5, Total Steps: 200, Ep Step: 30, Raw Reward: 10.00, Epsilon: 0.99\n",
      "Episode: 6, Total Steps: 217, Ep Step: 17, Raw Reward: -5.00, Epsilon: 0.99\n",
      "Episode: 7, Total Steps: 238, Ep Step: 21, Raw Reward: 10.00, Epsilon: 0.99\n",
      "Episode: 8, Total Steps: 256, Ep Step: 18, Raw Reward: 10.00, Epsilon: 0.99\n",
      "Episode: 9, Total Steps: 341, Ep Step: 85, Raw Reward: -5.00, Epsilon: 0.99\n",
      "Episode: 10, Total Steps: 350, Ep Step: 9, Raw Reward: -5.00, Epsilon: 0.99\n",
      "Episode: 11, Total Steps: 353, Ep Step: 3, Raw Reward: -5.00, Epsilon: 0.99\n",
      "Episode: 12, Total Steps: 354, Ep Step: 1, Raw Reward: -5.00, Epsilon: 0.99\n",
      "Episode: 13, Total Steps: 355, Ep Step: 1, Raw Reward: -5.00, Epsilon: 0.99\n",
      "Episode: 14, Total Steps: 356, Ep Step: 1, Raw Reward: -5.00, Epsilon: 0.99\n",
      "Episode: 15, Total Steps: 367, Ep Step: 11, Raw Reward: 10.00, Epsilon: 0.98\n",
      "Episode: 16, Total Steps: 384, Ep Step: 17, Raw Reward: 10.00, Epsilon: 0.98\n",
      "Episode: 17, Total Steps: 398, Ep Step: 14, Raw Reward: -5.00, Epsilon: 0.98\n",
      "Episode: 18, Total Steps: 403, Ep Step: 5, Raw Reward: 10.00, Epsilon: 0.98\n",
      "Episode: 19, Total Steps: 484, Ep Step: 81, Raw Reward: 10.00, Epsilon: 0.98\n",
      "Episode: 20, Total Steps: 510, Ep Step: 26, Raw Reward: 10.00, Epsilon: 0.98\n",
      "Episode: 21, Total Steps: 564, Ep Step: 54, Raw Reward: 10.00, Epsilon: 0.98\n",
      "Episode: 22, Total Steps: 603, Ep Step: 39, Raw Reward: 10.00, Epsilon: 0.98\n",
      "Episode: 23, Total Steps: 612, Ep Step: 9, Raw Reward: -5.00, Epsilon: 0.98\n",
      "Episode: 24, Total Steps: 634, Ep Step: 22, Raw Reward: 10.00, Epsilon: 0.98\n",
      "Episode: 25, Total Steps: 638, Ep Step: 4, Raw Reward: 10.00, Epsilon: 0.97\n",
      "Episode: 26, Total Steps: 645, Ep Step: 7, Raw Reward: 10.00, Epsilon: 0.97\n",
      "Episode: 27, Total Steps: 655, Ep Step: 10, Raw Reward: 10.00, Epsilon: 0.97\n",
      "Episode: 28, Total Steps: 699, Ep Step: 44, Raw Reward: 10.00, Epsilon: 0.97\n",
      "Episode: 29, Total Steps: 737, Ep Step: 38, Raw Reward: 10.00, Epsilon: 0.97\n",
      "Episode: 30, Total Steps: 749, Ep Step: 12, Raw Reward: 10.00, Epsilon: 0.97\n",
      "Episode: 31, Total Steps: 755, Ep Step: 6, Raw Reward: 10.00, Epsilon: 0.97\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T10:16:35.093256Z",
     "start_time": "2024-05-24T10:16:35.080906Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d18be3e342af45de",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "54df19849e24baeb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
